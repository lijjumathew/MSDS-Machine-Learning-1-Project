{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import basic packages lik numpy, pandas, math plot and seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas_profiling as pr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from datetime import datetime\n",
    "#import lightgbm as lgbm\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "warnings.filterwarnings('ignore') #ignore warning messages \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix,  roc_curve, precision_recall_curve, accuracy_score, roc_auc_score\n",
    "\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "import time\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, fbeta_score\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, ShuffleSplit, cross_validate, GridSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression, RFE\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, LassoCV, RidgeCV, ElasticNetCV, ElasticNet\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as tls\n",
    "import plotly.offline as py\n",
    "warnings.filterwarnings('ignore') #ignore warning messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows     :  7043\n",
      "Columns  :  21\n",
      "Features :  ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/lijjumathew/MSDS-Machine-Learning-1-Project/master/dataset/Telco-Customer-Churn.csv')\n",
    "\n",
    "# data overview\n",
    "print ('Rows     : ', df.shape[0])\n",
    "print ('Columns  : ', df.shape[1])\n",
    "print ('Features : ', df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>687</th>\n",
       "      <th>4147</th>\n",
       "      <th>2956</th>\n",
       "      <th>1105</th>\n",
       "      <th>2117</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>customerID</th>\n",
       "      <td>0067-DKWBL</td>\n",
       "      <td>3836-FZSDJ</td>\n",
       "      <td>5649-RXQTV</td>\n",
       "      <td>7363-QTBIW</td>\n",
       "      <td>4139-DETXS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partner</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenure</th>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhoneService</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultipleLines</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InternetService</th>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>DSL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OnlineBackup</th>\n",
       "      <td>No</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeviceProtection</th>\n",
       "      <td>No</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TechSupport</th>\n",
       "      <td>No</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StreamingTV</th>\n",
       "      <td>No</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StreamingMovies</th>\n",
       "      <td>No</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contract</th>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Two year</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Two year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaymentMethod</th>\n",
       "      <td>Electronic check</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <td>49.25</td>\n",
       "      <td>24.85</td>\n",
       "      <td>99</td>\n",
       "      <td>79.75</td>\n",
       "      <td>64.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalCharges</th>\n",
       "      <td>91.1</td>\n",
       "      <td>1901</td>\n",
       "      <td>5038.15</td>\n",
       "      <td>769.1</td>\n",
       "      <td>4528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Churn</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              687                        4147  \\\n",
       "customerID              0067-DKWBL                 3836-FZSDJ   \n",
       "gender                        Male                       Male   \n",
       "SeniorCitizen                    1                          1   \n",
       "Partner                         No                        Yes   \n",
       "Dependents                      No                         No   \n",
       "tenure                           2                         71   \n",
       "PhoneService                   Yes                        Yes   \n",
       "MultipleLines                   No                        Yes   \n",
       "InternetService                DSL                         No   \n",
       "OnlineSecurity                 Yes        No internet service   \n",
       "OnlineBackup                    No        No internet service   \n",
       "DeviceProtection                No        No internet service   \n",
       "TechSupport                     No        No internet service   \n",
       "StreamingTV                     No        No internet service   \n",
       "StreamingMovies                 No        No internet service   \n",
       "Contract            Month-to-month                   Two year   \n",
       "PaperlessBilling               Yes                         No   \n",
       "PaymentMethod     Electronic check  Bank transfer (automatic)   \n",
       "MonthlyCharges               49.25                      24.85   \n",
       "TotalCharges                  91.1                       1901   \n",
       "Churn                          Yes                         No   \n",
       "\n",
       "                              2956              1105  \\\n",
       "customerID              5649-RXQTV        7363-QTBIW   \n",
       "gender                        Male            Female   \n",
       "SeniorCitizen                    0                 0   \n",
       "Partner                         No               Yes   \n",
       "Dependents                      No                No   \n",
       "tenure                          51                 9   \n",
       "PhoneService                   Yes               Yes   \n",
       "MultipleLines                   No                No   \n",
       "InternetService        Fiber optic       Fiber optic   \n",
       "OnlineSecurity                  No                No   \n",
       "OnlineBackup                   Yes                No   \n",
       "DeviceProtection               Yes                No   \n",
       "TechSupport                     No                No   \n",
       "StreamingTV                    Yes                No   \n",
       "StreamingMovies                Yes               Yes   \n",
       "Contract            Month-to-month    Month-to-month   \n",
       "PaperlessBilling               Yes               Yes   \n",
       "PaymentMethod     Electronic check  Electronic check   \n",
       "MonthlyCharges                  99             79.75   \n",
       "TotalCharges               5038.15             769.1   \n",
       "Churn                           No                No   \n",
       "\n",
       "                                       2117  \n",
       "customerID                       4139-DETXS  \n",
       "gender                               Female  \n",
       "SeniorCitizen                             0  \n",
       "Partner                                 Yes  \n",
       "Dependents                              Yes  \n",
       "tenure                                   72  \n",
       "PhoneService                             No  \n",
       "MultipleLines              No phone service  \n",
       "InternetService                         DSL  \n",
       "OnlineSecurity                          Yes  \n",
       "OnlineBackup                            Yes  \n",
       "DeviceProtection                        Yes  \n",
       "TechSupport                             Yes  \n",
       "StreamingTV                             Yes  \n",
       "StreamingMovies                         Yes  \n",
       "Contract                           Two year  \n",
       "PaperlessBilling                         No  \n",
       "PaymentMethod     Bank transfer (automatic)  \n",
       "MonthlyCharges                        64.45  \n",
       "TotalCharges                           4528  \n",
       "Churn                                    No  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "Our dataset is telecom data with below features\n",
    "\n",
    "- Customer demographic data - Gender, SeniorCitizen, Partner, Dependents\n",
    "- Subscribed services - PhoneService, MultipleLine, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies\n",
    "- Customer account information - CustomerID, Contract, PaperlessBilling, PaymentMethod, MonthlyCharges, TotalCharges, Tenure\n",
    "\n",
    "The two objectives for this lab\n",
    "\n",
    "1. Regression \n",
    "    - Estimate Monthly Charges - Target variable is MonthlyCharges, which is a continous variable.\n",
    "2. Classification \n",
    "    - Churn Prediction - Target variable is Churn, which has binary classes 1 and 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression - Data Cleanup & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.73463\n",
       "1    0.26537\n",
       "Name: Churn, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ideally SeniorCitizen column should be a factor, so let's convert 1,0 values to Yes,No and later we can label encode all factor columns\n",
    "df.SeniorCitizen=df.SeniorCitizen.apply(lambda x: 'Yes' if x==1 else 'No')\n",
    "\n",
    "# Getting rid of unwanted columns like Customer Id.\n",
    "if 'customerID' in df:\n",
    "    del df['customerID']\n",
    "\n",
    "# remove TotalCharges\n",
    "df=df.drop(['TotalCharges'], axis=1)\n",
    "\n",
    "# converting TotalCharges object dataset into numeric\n",
    "#df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors = 'coerce')\n",
    "\n",
    "# Replacing blank values with nulls.\n",
    "df=df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# Total charges has some blank values/missing values and needs to be imputed. Filling the missing values\n",
    "#df[\"TotalCharges\"].fillna(df[\"TotalCharges\"].mean(), inplace=True)\n",
    "\n",
    "# Consolidate MultipleLines attribute\n",
    "df['MultipleLines'] = df['MultipleLines'].replace('No phone service','No')\n",
    "\n",
    "# Change all values of 'No internet service' to 'No'\n",
    "df = df.replace('No internet service','No')\n",
    "\n",
    "# Replace all yes/no values with 1/0\n",
    "df = df.replace(to_replace=['Yes','No'], value=[1,0])\n",
    "\n",
    "# Create dummy variables in the entire dataset, this is one hot encoding\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# check the distribution\n",
    "df['Churn'].value_counts()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the MonthlyCharges column from the dataset\n",
    "y = df['MonthlyCharges'].values\n",
    "x = df.drop(columns = ['MonthlyCharges'])\n",
    "\n",
    "# Set features\n",
    "features = x.columns.values\n",
    "\n",
    "#Divide data into test and training splits\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.10, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_actual, y_predicted):\n",
    "    return np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "#Function for Mean Absolute Percentage Error (MAPE)\n",
    "#Adapted from - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "def mape(y_actual, y_predicted): \n",
    "    mask = y_actual != 0\n",
    "    return (np.fabs(y_actual - y_predicted)/y_actual)[mask].mean() * 100\n",
    "\n",
    "#Create scorers for rmse and mape functions\n",
    "mae_scorer = make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
    "rmse_scorer = make_scorer(score_func=rmse, greater_is_better=False)\n",
    "mape_scorer = make_scorer(score_func=mape, greater_is_better=False)\n",
    "\n",
    "#Make scorer array to pass into cross_validate() function for producing mutiple scores for each cv fold.\n",
    "errorScoring = {'MAE':  mae_scorer, \n",
    "                'RMSE': rmse_scorer,\n",
    "                'MAPE': mape_scorer\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateRegressionEstimator(regEstimator, x, y, cv):\n",
    "    \n",
    "    scores = cross_validate(regEstimator, x, y, scoring=errorScoring, cv=cv, return_train_score=True)\n",
    "\n",
    "    #cross val score sign-flips the outputs of MAE\n",
    "    # https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "    scores['test_MAE'] = scores['test_MAE'] * -1\n",
    "    scores['test_MAPE'] = scores['test_MAPE'] * -1\n",
    "    scores['test_RMSE'] = scores['test_RMSE'] * -1\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    maeAvg = scores['test_MAE'].mean()\n",
    "    print_str = \"The average MAE for all cv folds is: \\t\\t\\t {maeAvg:.5}\"\n",
    "    print(print_str.format(maeAvg=maeAvg))\n",
    "\n",
    "    #print mean test_MAPE for all folds\n",
    "    scores['test_MAPE'] = scores['test_MAPE']\n",
    "    mape_avg = scores['test_MAPE'].mean()\n",
    "    print_str = \"The average MAE percentage (MAPE) for all cv folds is: \\t {mape_avg:.5}\"\n",
    "    print(print_str.format(mape_avg=mape_avg))\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    RMSEavg = scores['test_RMSE'].mean()\n",
    "    print_str = \"The average RMSE for all cv folds is: \\t\\t\\t {RMSEavg:.5}\"\n",
    "    print(print_str.format(RMSEavg=RMSEavg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['MAE'] = scores['test_MAE']\n",
    "    scoresResults['MAPE'] = scores['test_MAPE']\n",
    "    scoresResults['RMSE'] = scores['test_RMSE']\n",
    "    return scoresResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function caps the predicted values for our regression at 150. The max value in our dataset is 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new estimator compatible for use with GridSearchCV() and cross_validate()\n",
    "# -  Cap predict function for LinearRegression between 0 and 150\n",
    "\n",
    "class CappedLinearRegression(LinearRegression):\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.clip(super(CappedLinearRegression, self).predict(x), 0, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression - Parameter Tuning Using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=99, test_size=0.1, train_size=None),\n",
       "             estimator=CappedLinearRegression(),\n",
       "             param_grid={'fit_intercept': (True, False),\n",
       "                         'normalize': (True, False)},\n",
       "             scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
    "linreg = CappedLinearRegression()\n",
    "parameters = {'normalize':(True,False), 'fit_intercept':(True,False)}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "regGridSearch = GridSearchCV(estimator=linreg\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CappedLinearRegression(normalize=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the parameterization of the best estimator\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression - Baseline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average MAE for all cv folds is: \t\t\t 0.78368\n",
      "The average MAE percentage (MAPE) for all cv folds is: \t 1.379\n",
      "The average RMSE for all cv folds is: \t\t\t 1.0248\n",
      "*********************************************************\n",
      "Cross Validation Fold Mean Error Scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839909</td>\n",
       "      <td>1.426827</td>\n",
       "      <td>1.093134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.755247</td>\n",
       "      <td>1.341425</td>\n",
       "      <td>1.008063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.777725</td>\n",
       "      <td>1.342619</td>\n",
       "      <td>1.027155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.774009</td>\n",
       "      <td>1.339207</td>\n",
       "      <td>1.027833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.827133</td>\n",
       "      <td>1.425904</td>\n",
       "      <td>1.048610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.808013</td>\n",
       "      <td>1.500555</td>\n",
       "      <td>1.053997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.738264</td>\n",
       "      <td>1.304459</td>\n",
       "      <td>0.971737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.760341</td>\n",
       "      <td>1.312848</td>\n",
       "      <td>1.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.744399</td>\n",
       "      <td>1.337702</td>\n",
       "      <td>0.973046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.811741</td>\n",
       "      <td>1.458631</td>\n",
       "      <td>1.041673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE      MAPE      RMSE\n",
       "0  0.839909  1.426827  1.093134\n",
       "1  0.755247  1.341425  1.008063\n",
       "2  0.777725  1.342619  1.027155\n",
       "3  0.774009  1.339207  1.027833\n",
       "4  0.827133  1.425904  1.048610\n",
       "5  0.808013  1.500555  1.053997\n",
       "6  0.738264  1.304459  0.971737\n",
       "7  0.760341  1.312848  1.002413\n",
       "8  0.744399  1.337702  0.973046\n",
       "9  0.811741  1.458631  1.041673"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create CappedLinearRegression predictions between 0 and $150 using the best parameters for our Linear Regression object\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateRegressionEstimator(regEstimator, x, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yhat Max:  115.037109375\n",
      "Yhat Mean:  64.76136850197004\n"
     ]
    }
   ],
   "source": [
    "regEstimator.fit(x, y)\n",
    "yhat = regEstimator.predict(x)\n",
    "print(\"Yhat Max: \", yhat.max())\n",
    "print(\"Yhat Mean: \", yhat.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression - Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:   48.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=99, test_size=0.1, train_size=None),\n",
       "             estimator=SVR(), n_jobs=-1,\n",
       "             param_grid={'C': [0.001, 0.1],\n",
       "                         'gamma': [0.038461538461538464, 0.1],\n",
       "                         'kernel': ['rbf', 'linear']},\n",
       "             scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Linear regression object and perform a grid search to find the best parameters\n",
    "reg = SVR()\n",
    "\n",
    "#Set up SVR parameters to test\n",
    "costs = [0.001, 0.1]\n",
    "defGamma = 1 / x.shape[1]  #This is the default value for the gamma parameter\n",
    "gammas = [defGamma, 0.1]\n",
    "kernels = ['rbf','linear']\n",
    "parameters = {'C': costs, 'gamma' : gammas, 'kernel': kernels}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=-1 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=0.1, gamma=0.038461538461538464, kernel='linear')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average MAE for all cv folds is: \t\t\t 0.79774\n",
      "The average MAE percentage (MAPE) for all cv folds is: \t 1.4183\n",
      "The average RMSE for all cv folds is: \t\t\t 1.0397\n",
      "*********************************************************\n",
      "Cross Validation Fold Mean Error Scores\n",
      "Yhat Max:  115.07104108865217\n",
      "Yhat Mean:  64.72797731824387\n"
     ]
    }
   ],
   "source": [
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = SVR(C=0.1, gamma=0.038461538461538464, kernel='linear')\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "EvaluateRegressionEstimator(regEstimator, x, y, cv)\n",
    "\n",
    "regEstimator.fit(x, y)\n",
    "yhat = regEstimator.predict(x)\n",
    "print(\"Yhat Max: \", yhat.max())\n",
    "print(\"Yhat Mean: \", yhat.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing support vector regression we can see that our error metrics are very similar. This model also predicts a similar monthly charge to the original regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression - Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 96 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 945 out of 960 | elapsed:    2.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=99, test_size=0.1, train_size=None),\n",
       "             estimator=ElasticNet(max_iter=10000, normalize=True,\n",
       "                                  precompute=True, random_state=0),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'alpha': [0.001, 0.1, 1, 10],\n",
       "                         'l1_ratio': [0.001, 0.01, 0.1, 0.5, 0.75, 1],\n",
       "                         'selection': ['cyclic', 'random'],\n",
       "                         'warm_start': [True, False]},\n",
       "             scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a regression object and perform a grid search to find the best parameters\n",
    "reg = ElasticNet(fit_intercept=True, normalize=True, precompute=True, copy_X=True\n",
    "          , max_iter=10000, tol=0.0001, random_state=0)\n",
    " \n",
    "#Test parameters\n",
    "l1_ratio = [0.001, 0.01, 0.1, 0.5, 0.75, 1]\n",
    "alpha = [0.001, 0.1, 1, 10]\n",
    "selection = ['cyclic','random']\n",
    "warm_start = [True, False]\n",
    "parameters = {'l1_ratio': l1_ratio, 'alpha': alpha, 'selection': selection, 'warm_start': warm_start}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=-1 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.001, l1_ratio=1, max_iter=10000, normalize=True,\n",
       "           precompute=True, random_state=0, selection='random',\n",
       "           warm_start=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average MAE for all cv folds is: \t\t\t 0.79105\n",
      "The average MAE percentage (MAPE) for all cv folds is: \t 1.4071\n",
      "The average RMSE for all cv folds is: \t\t\t 1.0337\n",
      "*********************************************************\n",
      "Cross Validation Fold Mean Error Scores\n",
      "Yhat Max:  114.61039542315444\n",
      "Yhat Mean:  64.76169246059918\n"
     ]
    }
   ],
   "source": [
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = ElasticNet(alpha=0.001, l1_ratio=1, max_iter=10000, normalize=True,\n",
    "           precompute=True, random_state=0, warm_start=True)\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "EvaluateRegressionEstimator(regEstimator, x, y, cv)\n",
    "\n",
    "regEstimator.fit(x, y)\n",
    "yhat = regEstimator.predict(x)\n",
    "print(\"Yhat Max: \", yhat.max())\n",
    "print(\"Yhat Mean: \", yhat.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression - Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d1f616ab2e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#Perform hyperparameter search to find the best combination of parameters for our data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mregGridSearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
    "linreg = RandomForestRegressor()\n",
    "parameters = { 'min_samples_split':[2,3,4,5]\n",
    "              ,'n_estimators' : [500]\n",
    "              ,'min_samples_leaf': [10, 25, 50]\n",
    "              ,'criterion': ['mae']\n",
    "              ,'n_jobs':[16] \n",
    "              ,'random_state': [0]\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "regGridSearch = GridSearchCV(estimator=linreg\n",
    "                   , n_jobs=-1 \n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regEstimator = RandomForestRegressor(criterion='mae', min_samples_leaf=10, n_estimators=500,\n",
    "                      n_jobs=8, random_state=0)\n",
    "\n",
    "EvaluateRegressionEstimator(regEstimator, x, y, cv)\n",
    "\n",
    "regEstimator.fit(x, y)\n",
    "yhat = regEstimator.predict(x)\n",
    "print(\"Yhat Max: \", yhat.max())\n",
    "print(\"Yhat Mean: \", yhat.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite being the most computationally expensive, the random trees regression method provides the worst error metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression - Check Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load the model's coefficient weights and feature names into a dataframe sorted by weights\n",
    "weights = regEstimator.feature_importances_.ravel()\n",
    "feature_names = x.columns.values\n",
    "linreg_ft_imp_df = pd.DataFrame({'feature_names':feature_names, 'weights':weights, 'absolute_weights': np.abs(weights)})\n",
    "linreg_ft_imp_df.sort_values(by='absolute_weights', inplace=True, ascending=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine categorical variables of interest  \n",
    "#Plot the model's feature importances\n",
    "# REFERENCE:  Eric Larson, https://github.com/eclarson/DataMiningNotebooks\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "wt_plt_df = linreg_ft_imp_df.head(75)\n",
    "\n",
    "weights = pd.Series(wt_plt_df['weights'].values,index=wt_plt_df['feature_names'])\n",
    "ax = weights.plot(kind='bar', figsize=(20,8))\n",
    "\n",
    "ax.set_title(\"Top Feature Correlations\")\n",
    "ax.set_ylabel(\"Coefficient Magnitude\\n(z-score)\")\n",
    "ax.set_xlabel(\"Feature Names\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression - Conclusion\n",
    "\n",
    "- We mixed the data using 10-fold cross validation and a 90/10 train and test split. We used the same random seed for each model to keep the 90/10 split constant.\n",
    "- Our validation metrics consist of mean absolute error (MAE), mean absolute percentage error (MAPE), and root mean squared error (RMSE). Each of these metrics are wrapped in a single function so each regression model can be tested in the same manner.\n",
    "- Each regression function creates a model for every combination of parameters that are given to it times the number of folds in our cross validated data. In our first model we used a grid search with two parameters consisting of two options each. Combined with 10 different folds in the data, the grid search method returns 40 different models and a \"best estimator\" function call. We used this as our baseline analysis to compare against three more regression functions: Support Vector Regression, Elastic Net Regression, and Random Forest Regression.\n",
    "- The SVR and ENR regression methods returned similar error metrics to our baseline function while the Random Forest method returned higher a higher error rate despite being significantly more computationally expensive.\n",
    "- To conclude our analysis we checked the feature weights of the Random Forest model. Although not pictured in this report, the findings are very similar to the mini lab we completed previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification - Data Cleanup, Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resetting the data for Classification.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/lijjumathew/MSDS-Machine-Learning-1-Project/master/dataset/Telco-Customer-Churn.csv')\n",
    "\n",
    "#Ideally SeniorCitizen column should be a factor, so let's convert 1,0 values to Yes,No and later we can label encode all factor columns\n",
    "df.SeniorCitizen=df.SeniorCitizen.apply(lambda x: 'Yes' if x==1 else 'No')\n",
    "\n",
    "# Getting rid of unwanted columns like Customer Id.\n",
    "if 'customerID' in df:\n",
    "    del df['customerID']\n",
    "    \n",
    "# converting TotalCharges object dataset into numeric\n",
    "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors = 'coerce')\n",
    "\n",
    "# Replacing blank values with nulls.\n",
    "df=df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# Total charges has some blank values/missing values and needs to be imputed. Filling the missing values\n",
    "df[\"TotalCharges\"].fillna(df[\"TotalCharges\"].mean(), inplace=True)\n",
    "\n",
    "# Consolidate MultipleLines attribute\n",
    "df['MultipleLines'] = df['MultipleLines'].replace('No phone service','No')\n",
    "\n",
    "# Change all values of 'No internet service' to 'No'\n",
    "df = df.replace('No internet service','No')\n",
    "\n",
    "#Separating churn and non churn customers \n",
    "churn     = df[df[\"Churn\"] == \"Yes\"]\n",
    "not_churn = df[df[\"Churn\"] == \"No\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correlation Matrix for variables\n",
    "sns.set(rc={'figure.figsize':(8,6)})\n",
    "sns.heatmap(df.corr(), cmap=\"seismic\", annot=False, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There was missing values for the variable total charges and it was imputed taking mean.\n",
    "2. Variables like customer id was removed as it is not useful for this study.\n",
    "3. Few variables where values were more explantory like \"No Internet service\", \"No Phone Service\" were replaced with categorical values such as \"No\".\n",
    "4. From correlation matrix, Tenure and TotalCharges, Monthly and TotalCharges are corelated and it makes sense as totalcharges = tenure*MonthlyCharges. So Total charges is dropped from study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bin tenure in months as year range\n",
    "df['tenure_range'] = pd.cut(df.tenure,[0,12,24,36,48,60,72,84],3,\n",
    "                            labels=['1 year','2 year','3 year', ' 4 year', '5 year', '6 year', '7 year'])\n",
    "\n",
    "# cMonthlyCharges to categorical column \n",
    "def monthlycharges_split(df) : \n",
    " if df['MonthlyCharges'] <= 30 :\n",
    "     return '030'\n",
    " elif (df['MonthlyCharges'] > 30) & (df['MonthlyCharges'] <= 70 ):\n",
    "     return '3070'\n",
    " elif (df['MonthlyCharges'] > 70) & (df['MonthlyCharges'] <= 99 ):\n",
    "     return '7099'\n",
    " elif df['MonthlyCharges'] > 99 :\n",
    "     return '99plus'\n",
    "df['monthlycharges_bin'] = df.apply(lambda df:monthlycharges_split(df), axis = 1)\n",
    "\n",
    "# Getting rid of unwanted columns like Customer Id.\n",
    "if 'TotalCharges' in df:\n",
    "    del df['TotalCharges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# new features monthlycharges_bin\n",
    "plt.figure(figsize = [10,5])\n",
    "df[df.Churn == \"No\"]['monthlycharges_bin'].value_counts().plot(kind = 'bar', color=\"green\", alpha=0.5).set_title('monthlycharges_bin')\n",
    "df[df.Churn == \"Yes\"]['monthlycharges_bin'].value_counts().plot(kind = 'bar', color=\"red\", alpha=0.7, width=0.3)\n",
    "plt.legend(['No Churn', 'Churn'], shadow=True, loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# new features tenure_group\n",
    "plt.figure(figsize = [10,5])\n",
    "df[df.Churn == \"No\"]['tenure_range'].value_counts().plot(kind = 'bar', color=\"green\", alpha=0.5).set_title('tenure_group')\n",
    "df[df.Churn == \"Yes\"]['tenure_range'].value_counts().plot(kind = 'bar', color=\"red\", alpha=0.7, width=0.3)\n",
    "plt.legend(['No Churn', 'Churn'], shadow=True, loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Monthly charges were binned and we can see the churn is more when the monthly charges are above 70.\n",
    "2. Tenure is also binned and we see more churn in the first year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification -Dummy variables & Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convertin the predictor variable in a binary numeric variable\n",
    "df['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n",
    "df['Churn'].replace(to_replace='No',  value=0, inplace=True)\n",
    "\n",
    "#Let's convert all the categorical variables into dummy variables\n",
    "df_dummies = pd.get_dummies(df)\n",
    "y = df_dummies['Churn'].values\n",
    "X= df_dummies.drop(columns = ['Churn'])\n",
    "\n",
    "# Set features\n",
    "features = X.columns.values\n",
    "\n",
    "# Set up train/test split with 80/20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 99)\n",
    "\n",
    "# Normalize values\n",
    "scale = MinMaxScaler(feature_range = (0,1))\n",
    "scale.fit(X_train)\n",
    "X_train = pd.DataFrame(scale.transform(X_train))\n",
    "X_train.columns = features\n",
    "X_test = scale.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X1_train = sm.add_constant(X_train)  \n",
    "model = sm.OLS(y_train, X1_train)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## to find significant features using LassoCV (all X_scaled)\n",
    "\n",
    "print('Use LassoCV to find the optimal ALPHA value for L1 regularization')\n",
    "# Run the cross validation, find the best alpha, refit the model on all the data with that alpha\n",
    "alphavec = 10**np.linspace(-3,3,200)   # alpha varies from 0.001 to 1000\n",
    "lasso_model = LassoCV(alphas = alphavec, cv=5)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "# This is the best alpha value found\n",
    "print('LASSO best alpha: ', lasso_model.alpha_ )\n",
    "# display all coefficients in the model with optimal alpha\n",
    "list(zip(X.columns, lasso_model.coef_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame({\"Features\":X.columns,\"Lasso_Coefficients\":lasso_model.coef_})\n",
    "df_plot['Lasso_Coefficients']=df_plot['Lasso_Coefficients'].abs()\n",
    "f, axes = plt.subplots(figsize=(18, 20))\n",
    "sns.barplot(y = 'Features', x = 'Lasso_Coefficients', data=df_plot,order=df_plot.sort_values('Lasso_Coefficients',ascending = False).Features,color='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create decision tree classifer object\n",
    "rfc = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "\n",
    "# Train model, note that NO scaling is required\n",
    "model = rfc.fit(X_train, y_train)\n",
    "\n",
    "# Plot the top features based on its importance\n",
    "(pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "   .nlargest(47)   # can adjust based on how many top features you want\n",
    "   .plot(kind='barh', figsize=[20,15])\n",
    "    .invert_yaxis()) # Ensures that the feature with the most importance is on top, in descending order\n",
    "\n",
    "plt.yticks(size=15)\n",
    "plt.title('Top Features derived by Random Forest', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature selection we have used below techniques.\n",
    "1. OLS\n",
    "2. Lasso\n",
    "3. Random Forest\n",
    "\n",
    "Below are the top 5 features common to above feature selection techniques.\n",
    "1. Monthly Charges \n",
    "2. Tenure \n",
    "3. Contract\n",
    "4. Paper Billing\n",
    "5. Payment Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to report model summaries.\n",
    "def model_report(model_name, model):\n",
    "    print('\\nSearch for OPTIMAL THRESHOLD, vary from 0.0001 to 0.9999, fit/predict on train/test data')\n",
    "    model.fit(X_train, y_train)\n",
    "    optimal_th = 0.5   # start with default threshold value\n",
    "    \n",
    "    for i in range(0,3):\n",
    "        score_list = []\n",
    "        print('\\nLooping decimal place', i+1) \n",
    "        th_list = [np.linspace(optimal_th-0.4999, optimal_th+0.4999, 11), \n",
    "                  # eg [ 0.0001 , 0.1008, 0.2006, 0.3004, 0.4002, 0.5, 0.5998, 0.6996, 0.7994, 0.8992, 0.9999 ]\n",
    "                 np.linspace(optimal_th-0.1, optimal_th+0.1, 21), \n",
    "                  # eg 0.3xx [ 0.2 , 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ]\n",
    "                 np.linspace(optimal_th-0.01, optimal_th+0.01, 21)]\n",
    "                  # eg 0.30x [ 0.29 , 0.291, 0.292, 0.293, 0.294, 0.295, 0.296, 0.297, 0.298, 0.299, 0.3  , 0.301, 0.302, 0.303, 0.304, 0.305, 0.306, 0.307, 0.308, 0.309, 0.31 ]\n",
    "        for th in th_list[i]:\n",
    "            y_pred = (model.predict_proba(X_test)[:,1] >= th)\n",
    "            f1scor = f1_score(y_test, y_pred)\n",
    "            score_list.append(f1scor)\n",
    "            print('{:.3f}->{:.4f}'.format(th, f1scor), end=',  ')   # display score in 4 decimal pl\n",
    "        optimal_th = float(th_list[i][score_list.index(max(score_list))])\n",
    "\n",
    "    print('optimal F1 score = {:.4f}'.format(max(score_list)))\n",
    "    print('optimal threshold = {:.3f}'.format(optimal_th))\n",
    "\n",
    "    print(model_name, 'accuracy score is')\n",
    "    print('Training: {:.2f}%'.format(100*model.score(X_train, y_train)))  # score uses accuracy\n",
    "    print('Test set: {:.2f}%'.format(100*model.score(X_test, y_test)))   # should use cross validation\n",
    "\n",
    "    y_pred = (model.predict_proba(X_test)[:,1] >= 0.25)\n",
    "    print('\\nAdjust threshold to 0.25:')\n",
    "    print('Precision: {:.4f},   Recall: {:.4f},   F1 Score: {:.4f}'.format(\n",
    "        precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)))\n",
    "    print(model_name, 'confusion matrix: \\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('\\nDefault threshold of 0.50:')\n",
    "    print('Precision: {:.4f},   Recall: {:.4f},   F1 Score: {:.4f}'.format(\n",
    "        precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)))\n",
    "    print(model_name, 'confusion matrix: \\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    y_pred = (model.predict_proba(X_test)[:,1] >= 0.75)\n",
    "    print('\\nAdjust threshold to 0.75:')\n",
    "    print('Precision: {:.4f},   Recall: {:.4f},   F1 Score: {:.4f}'.format(\n",
    "        precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)))\n",
    "    print(model_name, 'confusion matrix: \\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    y_pred = (model.predict_proba(X_test)[:,1] >= optimal_th)\n",
    "    print('\\nOptimal threshold {:.3f}'.format(optimal_th))\n",
    "    print('Precision: {:.4f},   Recall: {:.4f},   F1 Score: {:.4f}'.format(\n",
    "        precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)))\n",
    "    print(model_name, 'confusion matrix: \\n', confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    global model_f1, model_auc, model_ll, model_roc_auc\n",
    "    model_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "    model_ll = log_loss(y_test, y_pred)\n",
    "    print(model_name, 'Log-loss: {:.4f}'.format(model_ll))\n",
    "    y_pred = model.predict(X_test)\n",
    "    model_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    print(model_name, 'roc_auc_score: {:.4f}'.format(model_roc_auc)) \n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    model_auc = auc(fpr, tpr)\n",
    "    print(model_name, 'AUC: {:.4f}'.format(model_auc))\n",
    "\n",
    "    # plot the ROC curve\n",
    "    plt.figure(figsize = [6,6])\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % model_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # plt.savefig('roc_auc_score')\n",
    "    plt.show()\n",
    "  \n",
    "    return\n",
    "\n",
    "# initialise lists to collect the results to plot later\n",
    "model_list = []\n",
    "f1_list = []\n",
    "auc_list = []\n",
    "ll_list = []\n",
    "roc_auc_list = []\n",
    "time_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n\"\"\"\"\"\" GaussianNB \"\"\"\"\"\"')\n",
    "time1 = time.time()\n",
    "gnb = GaussianNB()\n",
    "model_report('GaussianNB', gnb)\n",
    "\n",
    "model_list.append('GaussianNB')\n",
    "f1_list.append(model_f1)\n",
    "auc_list.append(model_auc)\n",
    "ll_list.append(model_ll)\n",
    "roc_auc_list.append(model_roc_auc)\n",
    "time_list.append(time.time() - time1)\n",
    "\n",
    "print('\\n\"\"\"\"\"\" BernoulliNB \"\"\"\"\"\"')\n",
    "time1 = time.time()\n",
    "bnb = BernoulliNB()\n",
    "model_report('BernoulliNB', bnb)\n",
    "\n",
    "model_list.append('BernoulliNB')\n",
    "f1_list.append(model_f1)\n",
    "auc_list.append(model_auc)\n",
    "ll_list.append(model_ll)\n",
    "roc_auc_list.append(model_roc_auc)\n",
    "time_list.append(time.time() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n\"\"\"\"\"\" LogisticRegression \"\"\"\"\"\"')\n",
    "print('\\nSearch for optimal hyperparameter C in LogisticRegresssion, vary C from 0.001 to 1000, using KFold(5) Cross Validation on train data')\n",
    "kf = KFold(n_splits=5, random_state=21, shuffle=True)  #produce the k folds\n",
    "score_list = []\n",
    "c_list = 10**np.linspace(-3,3,200)\n",
    "for c in c_list:\n",
    "    logit = LogisticRegression(C = c)\n",
    "    cvs = (cross_val_score(logit, X_train, y_train, cv=kf, scoring='f1')).mean()\n",
    "    score_list.append(cvs)\n",
    "    print('{:.4f}'.format(cvs), end=\", \")   # 4 decimal pl\n",
    "print('optimal cv F1 score = {:.4f}'.format(max(score_list)))\n",
    "optimal_c = float(c_list[score_list.index(max(score_list))])\n",
    "print('optimal value of C = {:.3f}'.format(optimal_c))\n",
    "\n",
    "time1 = time.time()\n",
    "logit = LogisticRegression(C = optimal_c)\n",
    "model_report('LogisticRegression', logit)\n",
    "\n",
    "model_list.append('LogisticRegression')\n",
    "f1_list.append(model_f1)\n",
    "auc_list.append(model_auc)\n",
    "ll_list.append(model_ll)\n",
    "roc_auc_list.append(model_roc_auc)\n",
    "time_list.append(time.time() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n\"\"\"\"\"\" KNN \"\"\"\"\"\" (quite slow)')\n",
    "print('\\nSearch for optimal hyperparameter K in KNN, vary K from 1 to 20, using KFold(5) Cross Validation on train data')\n",
    "kf = KFold(n_splits=5, random_state=21, shuffle=True)  #produce the k folds\n",
    "k_scores = []\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    cvs = cross_val_score(knn, X_train, y_train, cv=kf, scoring='f1').mean()\n",
    "    k_scores.append(cvs)\n",
    "    print('{:.4f}'.format(cvs), end=\", \")\n",
    "print('optimal cv F1 score = {:.4f}'.format(max(k_scores)))   # 4 decimal pl\n",
    "optimal_k = k_scores.index(max(k_scores))+1   # index 0 is for k=1\n",
    "print('optimal value of K =', optimal_k)\n",
    "\n",
    "time1 = time.time()\n",
    "knn = KNeighborsClassifier(n_neighbors = optimal_k)\n",
    "model_report('KNN', knn)\n",
    "\n",
    "print('\\nCompare with KNN classification_report (same as default threshold 0.50)')\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "model_list.append('KNN')\n",
    "f1_list.append(model_f1)\n",
    "auc_list.append(model_auc)\n",
    "ll_list.append(model_ll)\n",
    "roc_auc_list.append(model_roc_auc)\n",
    "time_list.append(time.time() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n\"\"\"\"\"\" DecisionTreeClassifier \"\"\"\"\"\"')\n",
    "\n",
    "print('\\nSearch for optimal max_depth in DecisionTree, vary 2 to 10, using KFold(5) Cross Validation on train data')\n",
    "kf = KFold(n_splits=5, random_state=21, shuffle=True)  #produce the k folds\n",
    "d_scores = []\n",
    "for d in range(2, 11):\n",
    "    decisiontree = DecisionTreeClassifier(max_depth=d)\n",
    "    cvs = cross_val_score(decisiontree, X_train, y_train, cv=kf, scoring='f1').mean()\n",
    "    d_scores.append(cvs)\n",
    "    print('{:.4f}'.format(cvs), end=\", \")\n",
    "print('optimal F1 score = {:.4f}'.format(max(d_scores)))   # 4 decimal pl\n",
    "optimal_d = d_scores.index(max(d_scores))+2   # index 0 is for d=2\n",
    "print('optimal max_depth =', optimal_d)\n",
    "\n",
    "time1 = time.time()\n",
    "decisiontree = DecisionTreeClassifier(max_depth=optimal_d)\n",
    "model_report('DecisionTreeClassifier', decisiontree)\n",
    "\n",
    "model_list.append('DecisionTreeClassifier')\n",
    "f1_list.append(model_f1)\n",
    "auc_list.append(model_auc)\n",
    "ll_list.append(model_ll)\n",
    "roc_auc_list.append(model_roc_auc)\n",
    "time_list.append(time.time() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n\"\"\"\"\"\" RandomForestClassifier \"\"\"\"\"\" (quite slow)')\n",
    "\n",
    "print('\\nSearch for optimal n_estimators in RandomForest, vary 100 to 500, using KFold(5) Cross Validation on train data')\n",
    "kf = KFold(n_splits=5, random_state=21, shuffle=True)  #produce the k folds\n",
    "score_list = []\n",
    "n_list = []\n",
    "for n in [100, 150, 200, 250, 300, 350, 400, 450, 500]:\n",
    "    randomforest = RandomForestClassifier(n_estimators=n)\n",
    "    cvs = (cross_val_score(randomforest, X_train, y_train, cv=kf, scoring='f1')).mean()\n",
    "    score_list.append(cvs)\n",
    "    n_list.append(n)\n",
    "    print('{:.0f}->{:.4f}'.format(n, cvs), end=\", \")   # display score in 4 decimal place\n",
    "print('optimal F1 score = {:.4f}'.format(max(score_list)))\n",
    "optimal_n = int(n_list[score_list.index(max(score_list))])\n",
    "print('optimal n_estimators = {:.0f}'.format(optimal_n))\n",
    "\n",
    "time1 = time.time()\n",
    "randomforest = RandomForestClassifier(n_estimators=optimal_n)\n",
    "model_report('RandomForestClassifier', randomforest)\n",
    "\n",
    "model_list.append('RandomForestClassifier')\n",
    "f1_list.append(model_f1)\n",
    "auc_list.append(model_auc)\n",
    "ll_list.append(model_ll)\n",
    "roc_auc_list.append(model_roc_auc)\n",
    "time_list.append(time.time() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n\"\"\"\"\"\" LinearSVC \"\"\"\"\"\"')\n",
    "time1 = time.time()\n",
    "linearsvc = LinearSVC()\n",
    "# model_report('LinearSVC', linearsvc)   # model has no attribute 'predict_proba'\n",
    "linearsvc.fit(X_train, y_train)\n",
    "print('LinearSVC accuracy score is')\n",
    "print('Training: {:.2f}%'.format(100*linearsvc.score(X_train, y_train)))  # score uses accuracy\n",
    "print('Test set: {:.2f}%'.format(100*linearsvc.score(X_test, y_test)))   # should use cross validation\n",
    "\n",
    "y_pred = linearsvc.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('LinearSVC confusion matrix: \\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "model_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "model_ll = log_loss(y_test, y_pred)\n",
    "print('LinearSVC Log-loss: {:.4f}'.format(model_ll))\n",
    "model_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print('LinearSVC roc_auc_score: {:.4f}'.format(model_roc_auc)) \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "model_auc = auc(fpr, tpr)\n",
    "print('LinearSVC AUC: {:.4f}'.format(model_auc))\n",
    "\n",
    "# plot the ROC curve\n",
    "plt.figure(figsize = [6,6])\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % model_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('roc_auc_score')\n",
    "plt.show()\n",
    "\n",
    "model_list.append('LinearSVC')\n",
    "f1_list.append(model_f1)\n",
    "auc_list.append(model_auc)\n",
    "ll_list.append(model_ll)\n",
    "roc_auc_list.append(model_roc_auc)\n",
    "time_list.append(time.time() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n\"\"\"\"\"\" SVC \"\"\"\"\"\" (extremely slow)')\n",
    "time1 = time.time()\n",
    "svc = SVC(gamma='scale', probability=True)\n",
    "model_report('SVC', svc)\n",
    "\n",
    "model_list.append('SVC')\n",
    "f1_list.append(model_f1)\n",
    "auc_list.append(model_auc)\n",
    "ll_list.append(model_ll)\n",
    "roc_auc_list.append(model_roc_auc)\n",
    "# time_list.append(time.time() - time1)   # use this line for actual time spent, or\n",
    "time_list.append(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## plot the classification report scores\n",
    "fig, ax = plt.subplots(5, 1, figsize=(18, 28))\n",
    "# fig.set_figwidth(10)\n",
    "# fig.set_figheight(6)\n",
    "# fig.suptitle('Main Title',fontsize = 16)\n",
    "ax[0].bar(model_list, f1_list)\n",
    "ax[0].set_title('F1-score')\n",
    "ax[1].bar(model_list, auc_list)\n",
    "ax[1].set_title('AUC-score')\n",
    "ax[2].bar(model_list, ll_list)\n",
    "ax[2].set_title('Log-Loss-Score')\n",
    "ax[3].bar(model_list, roc_auc_list)\n",
    "ax[3].set_title('ROC AUC Score')\n",
    "ax[4].bar(model_list, time_list)\n",
    "ax[4].set_title('Time taken')\n",
    "# Fine-tune figure: make subplots farther from each other, or nearer to each other.\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the ROC curves\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "y_pred = gnb.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, color='blue',\n",
    "        lw=3, label='GaussianNB (area = %0.2f)' % auc_list[0])\n",
    "\n",
    "y_pred = bnb.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, color='green',\n",
    "        lw=3, label='BernoulliNB (area = %0.2f)' % auc_list[1])\n",
    "\n",
    "y_pred = logit.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, color='red',\n",
    "        lw=2, label='LogisticRegression (area = %0.2f)' % auc_list[2])\n",
    "\n",
    "y_pred = knn.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, color='yellow',\n",
    "        lw=3, label='KNN (area = %0.2f)' % auc_list[3])\n",
    "\n",
    "y_pred = decisiontree.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, color='purple',\n",
    "        lw=2, label='DecisionTree (area = %0.2f)' % auc_list[4])\n",
    "\n",
    "y_pred = randomforest.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, color='brown',\n",
    "        lw=2, label='RandomForest (area = %0.2f)' % auc_list[5])\n",
    "\n",
    "y_pred = linearsvc.predict(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, color='cyan',\n",
    "        lw=2, label='LinearSVC (area = %0.2f)' % auc_list[6])\n",
    "\n",
    "y_pred = svc.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, color='magenta',\n",
    "        lw=2, label='SVC (area = %0.2f)' % auc_list[7])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('Receiver Operating Characteristic', fontsize=17)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Additional Analysis.\n",
    "# see how logistic regression confusion matrix varies with threshold\n",
    "def make_confusion_matrix(model, threshold=0.5):\n",
    "    # Predict class 1 if probability of being in class 1 is greater than threshold\n",
    "    # (model.predict(X_test) does this automatically with a threshold of 0.5)\n",
    "    y_pred = (logit.predict_proba(X_test)[:, 1] >= threshold)\n",
    "    conf = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize = [5,5])\n",
    "    sns.heatmap(conf, cmap=plt.cm.Blues, annot=True, square=True, fmt='d',\n",
    "           xticklabels=['no churn', 'churn'],\n",
    "           yticklabels=['no churn', 'churn'])\n",
    "    plt.xlabel('prediction')\n",
    "    plt.ylabel('actual')\n",
    "# Let's see how our confusion matrix changes with changes to the cutoff! \n",
    "from ipywidgets import interactive, FloatSlider\n",
    "logit = LogisticRegression(C = optimal_c)\n",
    "logit.fit(X_train, y_train)\n",
    "interactive(lambda threshold: make_confusion_matrix(logit, threshold), threshold=(0.0,1.0,0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Conclusion\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "After feature selection, we ran the below 8 models, trained and optimized them.\n",
    "1. Gaussian NB\n",
    "2. Bernoulli NB\n",
    "3. Logistric Regression\n",
    "4. KNN\n",
    "5. Decision Tree\n",
    "6. RandomForest\n",
    "7. LinearSVC\n",
    "8. SVC\n",
    "\n",
    "### Metrics\n",
    "\n",
    "We have used 3 metrics for model selection. The below metrics are chosen because our goal is to predict customer who are likely to churn with better accuracy.\n",
    "1. F-Score - which tells the balance between Precision and Recall rates trade off.\n",
    "2. ROC - which  is a relationship between true positive rate and false positive rate.\n",
    "3. Log Loss - which measures uncertainity.\n",
    "\n",
    "### Final Model\n",
    "Logistic Regression performed better than the rest. It had better F1-scores, highest ROC value and one with lowest loss function.\n",
    "\n",
    "### Additional Analysis\n",
    "We have already done Grid search methods in the mini lab and it provided wieghted search provided an optimal model. In this lab we are providing a widget where threshold can be tuned to Logistic regression to maximize the F1-scores. By tuning this hyper parameter, we achieved the optimized recall rate of 76%.\n",
    "\n",
    "### Deployment\n",
    "These models can generate a list of customers who are most vulnerable to churn, so that a focused customer retention program can be prioritized on them.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
